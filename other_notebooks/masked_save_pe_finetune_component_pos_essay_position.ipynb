{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b739c74",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "031a2430-7ba4-431f-8e42-e31baf9ce991",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas==1.3.4 in /opt/conda/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers==4.12.5 in /opt/conda/lib/python3.8/site-packages (4.12.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.9.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (2.26.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (1.21.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2021.5.30)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (8.0.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets==1.15.1 in /opt/conda/lib/python3.8/site-packages (1.15.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (4.62.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.70.13)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (1.3.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (1.21.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.9.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (2022.7.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (21.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (9.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==1.15.1) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (6.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==1.15.1) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==1.15.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.15.1) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.0.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.2)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.3.4\n",
    "!pip install transformers==4.12.5\n",
    "!pip install datasets==1.15.1\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e608c4dc",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "db736e14-03fb-4438-938d-c705d6786666",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832f569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb4b2bc3",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "37286c02-6bf4-41f5-ab70-51515a054e7b",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3baff1",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "0c699094-439f-4dda-85a9-815e7948540c",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/notebooks/Data/bert_sequence_classification'\n",
    "DATA_FILE = '/notebooks/linguistic_features/data/hf_datasets/pe_dataset_w_essay_position_pos_tags.pt'\n",
    "RESULTS_FOLDER = '/notebooks/Results/bert_sequence_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9891fdca",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "35aada91-232a-421e-a28f-94b359c6d65d",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc072ef",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "dc52e71e-65fa-4946-acdf-e4fffe9d0f79",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44451ebb",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d352c1cd-abff-4b1e-b5e2-611288e4fddb",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eba52554",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "013a6d64-65e7-4189-a468-40ecfd8a6736",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f57179",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "626737f9-ff56-4992-b72b-60750375e455",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "dataset = torch.load(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e859c93",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "4fbfcc7b-55fb-456e-ab02-2ae975803046",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['essay_nr', 'component_id', 'label_and_comp_idxs', 'text', 'label_x', 'label_ComponentType', 'relation_SupportAttack', 'label_RelationType', 'label_LinkedNotLinked', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'sentence', 'paragraph', 'para_nr', 'total_paras', 'token_count', 'token_count_covering_para', 'tokens_count_covering_sentence', 'preceeding_tokens_in_sentence_count', 'succeeding_tokens_in_sentence_count', 'token_ratio', 'relative_position_in_para_char', 'is_in_intro', 'relative_position_in_para_token', 'is_in_conclusion', 'is_first_in_para', 'is_last_in_para', 'nr_preceeding_comps_in_para', 'nr_following_comps_in_para', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'para_ratio', 'first_or_last', 'strct_fts_w_position_in_essay', 'component_pos_tags', 'strct_fts_essay_position_pos_tags'],\n",
       "        num_rows: 3770\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['essay_nr', 'component_id', 'label_and_comp_idxs', 'text', 'label_x', 'label_ComponentType', 'relation_SupportAttack', 'label_RelationType', 'label_LinkedNotLinked', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'sentence', 'paragraph', 'para_nr', 'total_paras', 'token_count', 'token_count_covering_para', 'tokens_count_covering_sentence', 'preceeding_tokens_in_sentence_count', 'succeeding_tokens_in_sentence_count', 'token_ratio', 'relative_position_in_para_char', 'is_in_intro', 'relative_position_in_para_token', 'is_in_conclusion', 'is_first_in_para', 'is_last_in_para', 'nr_preceeding_comps_in_para', 'nr_following_comps_in_para', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'para_ratio', 'first_or_last', 'strct_fts_w_position_in_essay', 'component_pos_tags', 'strct_fts_essay_position_pos_tags'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['essay_nr', 'component_id', 'label_and_comp_idxs', 'text', 'label_x', 'label_ComponentType', 'relation_SupportAttack', 'label_RelationType', 'label_LinkedNotLinked', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'sentence', 'paragraph', 'para_nr', 'total_paras', 'token_count', 'token_count_covering_para', 'tokens_count_covering_sentence', 'preceeding_tokens_in_sentence_count', 'succeeding_tokens_in_sentence_count', 'token_ratio', 'relative_position_in_para_char', 'is_in_intro', 'relative_position_in_para_token', 'is_in_conclusion', 'is_first_in_para', 'is_last_in_para', 'nr_preceeding_comps_in_para', 'nr_following_comps_in_para', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'para_ratio', 'first_or_last', 'strct_fts_w_position_in_essay', 'component_pos_tags', 'strct_fts_essay_position_pos_tags'],\n",
       "        num_rows: 943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcb5bf98",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 7,
     "id": "57c21a2e-83cd-4f4b-a19a-0bae29eb4794",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic: Gender Equality at university admission, Sentence: Therefore, universities follow the requirement of job providers and decide subject suitable for particular gender., First or last in essay: No, First in paragraph: No, Last in paragraph: Yes, In in introduction: No, Is in conclusion: No. Part Of Speech tags: NOUN, VERB, DET, NOUN, ADP, NOUN, NOUN, CCONJ, VERB, NOUN, ADJ, ADP, ADJ, NOUN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['strct_fts_essay_position_pos_tags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c005c1c6",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "492e0415-2634-47bb-88bf-665c130d032c",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeca6187",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 9,
     "id": "ba2c9132-d013-4b26-be96-146cf024a45e",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "label_names = set(dataset['train']['label_ComponentType'])\n",
    "label_nb = len(label_names)\n",
    "labels = ClassLabel(num_classes=label_nb, names=label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99718666",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 10,
     "id": "2f1ce51b-18bc-40db-983f-3434b8651e45",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=3, names={'Claim', 'MajorClaim', 'Premise'}, names_file=None, id=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8804c27e",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 11,
     "id": "5438b78c-249c-48ac-98e8-a87a2cc0c86d",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch['strct_fts_essay_position_pos_tags'], truncation=True, padding=True, max_length=512)\n",
    "    tokens['labels'] = labels.str2int(batch['label_ComponentType'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "008e8f5a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 12,
     "id": "cb8d8c9a-7539-4d02-9dc5-98bade787549",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x7f928e0728b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019246816635131836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5f8043087243ec91a0d6670d7b0fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017469406127929688,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62613bf1e464bc3bf093bbfa9b9e51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018579483032226562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd787a4b4fe4470bb5b896b7f0ccd3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2727dcf8",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 13,
     "id": "84abf830-f842-4e91-8d5c-7fcd9ea2942e",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b514adec",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 14,
     "id": "c7d70e98-c2e0-4055-9d52-74c67c199c72",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['argument_bound_1', 'argument_bound_2', 'argument_id', 'attention_mask', 'component_id', 'component_pos_tags', 'essay', 'essay_nr', 'first_or_last', 'input_ids', 'is_first_in_para', 'is_in_conclusion', 'is_in_intro', 'is_last_in_para', 'label_ComponentType', 'label_LinkedNotLinked', 'label_RelationType', 'label_and_comp_idxs', 'label_x', 'labels', 'nr_following_comps_in_para', 'nr_preceeding_comps_in_para', 'para_nr', 'para_ratio', 'paragraph', 'preceeding_tokens_in_sentence_count', 'relation_SupportAttack', 'relative_position_in_para_char', 'relative_position_in_para_token', 'sentence', 'split', 'strct_fts_essay_position_pos_tags', 'strct_fts_w_position_in_essay', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'succeeding_tokens_in_sentence_count', 'text', 'token_count', 'token_count_covering_para', 'token_ratio', 'token_type_ids', 'tokens_count_covering_sentence', 'total_paras'],\n",
       "        num_rows: 3770\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['argument_bound_1', 'argument_bound_2', 'argument_id', 'attention_mask', 'component_id', 'component_pos_tags', 'essay', 'essay_nr', 'first_or_last', 'input_ids', 'is_first_in_para', 'is_in_conclusion', 'is_in_intro', 'is_last_in_para', 'label_ComponentType', 'label_LinkedNotLinked', 'label_RelationType', 'label_and_comp_idxs', 'label_x', 'labels', 'nr_following_comps_in_para', 'nr_preceeding_comps_in_para', 'para_nr', 'para_ratio', 'paragraph', 'preceeding_tokens_in_sentence_count', 'relation_SupportAttack', 'relative_position_in_para_char', 'relative_position_in_para_token', 'sentence', 'split', 'strct_fts_essay_position_pos_tags', 'strct_fts_w_position_in_essay', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'succeeding_tokens_in_sentence_count', 'text', 'token_count', 'token_count_covering_para', 'token_ratio', 'token_type_ids', 'tokens_count_covering_sentence', 'total_paras'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['argument_bound_1', 'argument_bound_2', 'argument_id', 'attention_mask', 'component_id', 'component_pos_tags', 'essay', 'essay_nr', 'first_or_last', 'input_ids', 'is_first_in_para', 'is_in_conclusion', 'is_in_intro', 'is_last_in_para', 'label_ComponentType', 'label_LinkedNotLinked', 'label_RelationType', 'label_and_comp_idxs', 'label_x', 'labels', 'nr_following_comps_in_para', 'nr_preceeding_comps_in_para', 'para_nr', 'para_ratio', 'paragraph', 'preceeding_tokens_in_sentence_count', 'relation_SupportAttack', 'relative_position_in_para_char', 'relative_position_in_para_token', 'sentence', 'split', 'strct_fts_essay_position_pos_tags', 'strct_fts_w_position_in_essay', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'succeeding_tokens_in_sentence_count', 'text', 'token_count', 'token_count_covering_para', 'token_ratio', 'token_type_ids', 'tokens_count_covering_sentence', 'total_paras'],\n",
       "        num_rows: 943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7de10c60",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 15,
     "id": "e825c71a-23f8-4794-b114-729819def9ab",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']#.shuffle(seed=42)\n",
    "test_dataset = dataset['test']#.shuffle(seed=42)\n",
    "\n",
    "# train_val_datasets = dataset['train'].train_test_split(train_size=0.8, seed=42)\n",
    "# train_dataset = train_val_datasets['train']\n",
    "val_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8123775",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 16,
     "id": "e13433b1-343d-417d-bfbd-8bacb4c57d23",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "dataset_d = {}\n",
    "dataset_d['train'] = train_dataset\n",
    "dataset_d['test'] = test_dataset\n",
    "dataset_d['val'] = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a399b72",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 17,
     "id": "a298a871-1a0d-4ed8-9a1c-1ed3795f1d74",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] topic : zoos have no useful purpose?, sentence : in the zoo you can see an animal and their different variations, the male and the female or the baby and the adult, first or last in essay : no, first in paragraph : no, last in paragraph : no, in in introduction : no, is in conclusion : no. part of speech tags : adp, det, noun, pron, verb, verb, det, noun, cconj, det, adj, noun, punct, det, noun, cconj, det, noun, cconj, det, noun, cconj, det, noun [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset['train'][2945]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0b423bc",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 18,
     "id": "aa1384b3-bd47-41a3-86b1-9cf814d47cdf",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "set(dataset_d['train']['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d96cda23",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 19,
     "id": "84df366a-6334-4e3a-902b-deb1c99491fb",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "set(dataset_d['val']['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c26a2238",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 20,
     "id": "406ad783-a570-4c26-8aa6-243e866b6fbf",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TEST'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "set(dataset_d['test']['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f22259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "NUM_LABELS = labels.num_classes\n",
    "BATCH_SIZE = 48\n",
    "NB_EPOCHS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f9c3348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /notebooks/Prompting/notebooks/pe_mask_pipeline_model_new_trainer_corrected were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /notebooks/Prompting/notebooks/pe_mask_pipeline_model_new_trainer_corrected and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"/notebooks/Prompting/notebooks/pe_mask_pipeline_model_new_trainer_corrected\", num_labels=NUM_LABELS)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "474d9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(dataset_d['train']['labels'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7994a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [max(counter.values()) / counter[k] for k in sorted(counter.keys())]\n",
    "class_weights = torch.FloatTensor(class_weights)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "479fd4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5645, 5.2418, 1.0000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c753575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMacroF1(nn.Module):\n",
    "    \n",
    "    def __init__(self, weights):\n",
    "        super(SoftMacroF1, self).__init__();\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Computes differentiable macro F1 score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictions : torch.Tensor\n",
    "            tensor of predictions (float, 2D)\n",
    "\n",
    "        targets : torch.Tensor\n",
    "            tensor of targets (integers, 1D)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cost : torch.Tensor\n",
    "            1 - differentiable macro F1 (0D, i.e., value)\n",
    "        \"\"\"\n",
    "        dim = predictions.shape[1]\n",
    "        soft_f1s = torch.zeros(size=(dim,))\n",
    "\n",
    "        for i in range(dim):\n",
    "\n",
    "            targets_tmp = targets == i\n",
    "\n",
    "            targets_tmp = targets_tmp.float()\n",
    "            # predictions_i = predictions[:, i].float()\n",
    "\n",
    "            tp = torch.sum(predictions[:, i] * targets_tmp, axis=0)\n",
    "            fp = torch.sum(predictions[:, i] * (1 - targets_tmp), axis=0)\n",
    "            fn = torch.sum((1 - predictions[:, i]) * targets_tmp, axis=0)\n",
    "            soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "            soft_f1s[i] = soft_f1\n",
    "\n",
    "        macro_f1 = torch.mean(soft_f1s, axis=0)\n",
    "\n",
    "        cost = 1 - macro_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "\n",
    "        # macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8595755",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 27,
     "id": "521987db-56d5-4e71-95af-7d1fb4a06e3f",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/main_classes/trainer.html\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss_fct = SoftMacroF1(weights=class_weights) #nn.CrossEntropyLoss() #(weight=class_weights)SoftMacroF1()nn.CrossEntropyLoss()#(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba4f8c45",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 28,
     "id": "06697293-0ef4-4d24-95a9-6ca0ed569b51",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "metric = load_metric('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42cfdba5",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 29,
     "id": "956f8a45-f8bd-42a5-b829-5e2b0e82cf42",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \n",
    "    # output\n",
    "    output_dir=RESULTS_FOLDER,          \n",
    "    \n",
    "    # params\n",
    "    num_train_epochs=NB_EPOCHS,               # nb of epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # cf. paper Sun et al.\n",
    "    learning_rate=1e-5,#2e-5,                 # cf. paper Sun et al.\n",
    "#     warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "    warmup_ratio=0.1,                         # cf. paper Sun et al.\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    \n",
    "    # eval\n",
    "    evaluation_strategy=\"steps\",              # cf. paper Sun et al.\n",
    "    eval_steps=20,                            # cf. paper Sun et al.\n",
    "    \n",
    "    # log\n",
    "    logging_dir=\"/notebooks/Results/bert_sequence_classification/tb_logs\",  \n",
    "    logging_strategy='steps',\n",
    "    logging_steps=20,\n",
    "    \n",
    "    # save\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    # save_steps=20, # default 500\n",
    "    load_best_model_at_end=True,              # cf. paper Sun et al.\n",
    "    # metric_for_best_model='eval_loss' \n",
    "    metric_for_best_model='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09dfb047",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 30,
     "id": "219545f7-5aff-4d0c-87e2-4ca28a6e7acc",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer( # Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aced1a7b",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 31,
     "id": "b524a80a-ccf0-41f4-a015-e4ba2913fdc4",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running training *****\n",
      "  Num examples = 3770\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 13:22, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.696600</td>\n",
       "      <td>0.643007</td>\n",
       "      <td>0.260756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>0.621249</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.454400</td>\n",
       "      <td>1.123525</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>0.335920</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.494015</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.425300</td>\n",
       "      <td>0.317107</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.471748</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.409815</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>0.433636</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.352885</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.307103</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.464223</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.410800</td>\n",
       "      <td>0.431072</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.339269</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>0.320196</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.524160</td>\n",
       "      <td>0.225333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.437100</td>\n",
       "      <td>0.375787</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.408497</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.396600</td>\n",
       "      <td>0.254472</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.689825</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.443100</td>\n",
       "      <td>0.405338</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>0.393844</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.384677</td>\n",
       "      <td>0.251979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed71b881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed577a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nr_following_comps_in_para, relation_SupportAttack, argument_bound_1, strct_fts_w_position_in_essay, preceeding_tokens_in_sentence_count, label_RelationType, component_pos_tags, is_in_intro, label_x, succeeding_tokens_in_sentence_count, para_nr, first_or_last, relative_position_in_para_char, para_ratio, argument_bound_2, essay_nr, strct_fts_essay_position_pos_tags, label_ComponentType, label_LinkedNotLinked, is_in_conclusion, nr_preceeding_comps_in_para, component_id, token_count_covering_para, total_paras, structural_fts_as_text_combined, text, label_and_comp_idxs, essay, token_ratio, argument_id, is_first_in_para, paragraph, relative_position_in_para_token, sentence, split, token_count, tokens_count_covering_sentence, is_last_in_para, structural_fts_as_text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1260\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trainer = Trainer(model, data_collator=DataCollatorWithPadding(tokenizer))\n",
    "test_raw_preds, test_labels, _ = test_trainer.predict(test_dataset)\n",
    "test_preds = np.argmax(test_raw_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5163a473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1b6536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Claim      0.000     0.000     0.000       302\n",
      "  MajorClaim      0.000     0.000     0.000       153\n",
      "     Premise      0.639     1.000     0.780       805\n",
      "\n",
      "    accuracy                          0.639      1260\n",
      "   macro avg      0.213     0.333     0.260      1260\n",
      "weighted avg      0.408     0.639     0.498      1260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_name = labels.int2str([0,1,2])\n",
    "print(classification_report(test_labels, test_preds, target_names=target_name, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4dbc0135",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "     Premise      0.933     0.876     0.903       805\n",
    "  MajorClaim      0.794     0.935     0.859       153\n",
    "       Claim      0.670     0.719     0.693       302\n",
    "\n",
    "\n",
    "    accuracy                          0.845      1260\n",
    "   macro avg      0.799     0.843     0.818      1260\n",
    "weighted avg      0.853     0.845     0.848      1260\n",
    "\n",
    "with the masked saved model + strct_fts_essay_position_pos_tags field + normal loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6ddcf5b",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "     Premise      0.935     0.876     0.904       805\n",
    "       Claim      0.676     0.725     0.700       302\n",
    "  MajorClaim      0.797     0.948     0.866       153\n",
    "\n",
    "    accuracy                          0.848      1260\n",
    "   macro avg      0.803     0.850     0.823      1260\n",
    "weighted avg      0.856     0.848     0.851      1260"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6820a0a2",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "     Premise      0.934     0.883     0.908       805\n",
    "       Claim      0.677     0.748     0.711       302\n",
    "  MajorClaim      0.824     0.889     0.855       153\n",
    "\n",
    "    accuracy                          0.852      1260\n",
    "   macro avg      0.812     0.840     0.825      1260\n",
    "weighted avg      0.859     0.852     0.854      1260\n",
    "\n",
    "# OLD GOOD RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e228a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4684d865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f928c0a9550>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(test_labels, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2befaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
